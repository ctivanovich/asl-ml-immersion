{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objective:**\n",
    "\n",
    "* Learn how to adapt the tfx template to an existing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guided project, we will use the `tfx template` tool to create a TFX pipeline around the covertype dataset.\n",
    "The goal is to adapt the template pipeline to make use of the model code for the covertype dataset we  already developed in\n",
    "the first part of this course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envirnonment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the your Kubeflow pipelines endopoint below the same way you did in guided project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com/' # Enter your ENDPOINT here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=qwiklabs-gcp-04-0ad772141888\n"
     ]
    }
   ],
   "source": [
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'\n",
    "CUSTOM_TFX_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skaffold` tool setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/skaffold\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "LOCAL_BIN=\"/home/jupyter/.local/bin\"\n",
    "SKAFFOLD_URI=\"https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\"\n",
    "\n",
    "test -d $LOCAL_BIN || mkdir -p $LOCAL_BIN\n",
    "\n",
    "which skaffold || (\n",
    "    curl -Lo skaffold $SKAFFOLD_URI &&\n",
    "    chmod +x skaffold               &&\n",
    "    mv skaffold $LOCAL_BIN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `PATH` environment variable so that `skaffold` is available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you shoud see the `skaffold` tool with the command `which`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/skaffold\n"
     ]
    }
   ],
   "source": [
    "!which skaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Copy the predefined template to your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will create a working pipeline project directory and \n",
    "files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the PIPELINE_NAME below. \n",
    "\n",
    "This will also become the name of the project directory where your files will be put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./guided_project_2'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_NAME = \"guided_project_2\"\n",
    "PROJECT_DIR = os.path.join(os.path.expanduser(\".\"), PIPELINE_NAME)\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX includes the taxi template with the TFX python package. \n",
    "\n",
    "If you are planning to solve a point-wise prediction problem,\n",
    "including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 01:48:39.693926: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 01:48:39.693974: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Copying taxi pipeline template\n",
      "model_analysis.ipynb -> ./guided_project_2/model_analysis.ipynb\n",
      "pipeline.py -> ./guided_project_2/pipeline/pipeline.py\n",
      "__init__.py -> ./guided_project_2/pipeline/__init__.py\n",
      "configs.py -> ./guided_project_2/pipeline/configs.py\n",
      "kubeflow_dag_runner.py -> ./guided_project_2/kubeflow_dag_runner.py\n",
      "data_validation.ipynb -> ./guided_project_2/data_validation.ipynb\n",
      "model.py -> ./guided_project_2/models/keras/model.py\n",
      "constants.py -> ./guided_project_2/models/keras/constants.py\n",
      "__init__.py -> ./guided_project_2/models/keras/__init__.py\n",
      "model_test.py -> ./guided_project_2/models/keras/model_test.py\n",
      "model.py -> ./guided_project_2/models/estimator/model.py\n",
      "constants.py -> ./guided_project_2/models/estimator/constants.py\n",
      "__init__.py -> ./guided_project_2/models/estimator/__init__.py\n",
      "model_test.py -> ./guided_project_2/models/estimator/model_test.py\n",
      "preprocessing.py -> ./guided_project_2/models/preprocessing.py\n",
      "preprocessing_test.py -> ./guided_project_2/models/preprocessing_test.py\n",
      "__init__.py -> ./guided_project_2/models/__init__.py\n",
      "features.py -> ./guided_project_2/models/features.py\n",
      "features_test.py -> ./guided_project_2/models/features_test.py\n",
      "__init__.py -> ./guided_project_2/__init__.py\n",
      "beam_dag_runner.py -> ./guided_project_2/beam_dag_runner.py\n",
      ".gitignore -> ./guided_project_2/.gitignore\n"
     ]
    }
   ],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline-name={PIPELINE_NAME} \\\n",
    "  --destination-path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Browse your copied source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code,\n",
    "sample data, and Jupyter Notebooks to analyse the output of the pipeline. \n",
    "\n",
    "The `taxi` template uses the same Chicago Taxi dataset and ML model as \n",
    "the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "Here is brief introduction to each of the Python files:\n",
    "\n",
    "`pipeline` - This directory contains the definition of the pipeline\n",
    "* `configs.py` — defines common constants for pipeline runners\n",
    "* `pipeline.py` — defines TFX components and a pipeline\n",
    "\n",
    "`models` - This directory contains ML model definitions.\n",
    "* `features.py`, `features_test.py` — defines features for the model\n",
    "* `preprocessing.py`, `preprocessing_test.py` — defines preprocessing jobs using tf::Transform\n",
    "\n",
    "`models/estimator` - This directory contains an Estimator based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "\n",
    "`models/keras` - This directory contains a Keras based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "\n",
    "`beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n",
    "\n",
    "\n",
    "**Running the tests:**\n",
    "You might notice that there are some files with `_test.py` in their name. \n",
    "These are unit tests of the pipeline and it is recommended to add more unit \n",
    "tests as you implement your own pipelines. \n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. \n",
    "You can usually get a module name by deleting `.py` extension and replacing `/` with `..`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 03:58:31.844963: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 03:58:31.845007: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "INFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.0s\n",
      "I1112 03:58:35.159034 140376418719552 test_util.py:1973] time(__main__.ModelTest.testBuildKerasModel): 0.0s\n",
      "[  FAILED  ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "======================================================================\n",
      "ERROR: testBuildKerasModel (__main__.ModelTest)\n",
      "ModelTest.testBuildKerasModel\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2/models/keras/model_test.py\", line 29, in testBuildKerasModel\n",
      "    hidden_units=[1, 1], learning_rate=0.1)  # pylint: disable=protected-access\n",
      "TypeError: _build_keras_model() got an unexpected keyword argument 'hidden_units'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (errors=1, skipped=1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !python -m models.features_test\n",
    "!python -m models.keras.model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Create the artifact store bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You probably already have completed this step in guided project 1, so you may\n",
    "may skip it if this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components in the TFX pipeline will generate outputs for each run as\n",
    "[ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere.\n",
    "You can use any storage which the KFP cluster can access, and for this example we\n",
    "will use Google Cloud Storage (GCS).\n",
    "\n",
    "Let us create this bucket if you haven't created it in guided project 1.\n",
    "Its name will be `<YOUR_PROJECT>-kubeflowpipelines-default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-kubeflowpipelines-default'\n",
    "GCS_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/data/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/jobs/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/staging/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx-template/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx_pipeline_output/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tmp/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{GCS_BUCKET_NAME} | grep {GCS_BUCKET_NAME} || gsutil mb gs://{GCS_BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Change the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to have the TFX pipeline dataset point to the GCS bucket\n",
    "where our covertype data is located. For that\n",
    "\n",
    "1. open `kubeflow_dag_runner.py`\n",
    "\n",
    "1. Set the variable `DATA_PATH` to `gs://workshop-datasets/covertype/small` which contains the covertype dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Change the pre-processing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we want to reuse the `features.py` and `preprocessing.py` we already have for the covertype dataset. To do that, we will just copy these files over the template ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_PY  = '../../tfx_pipelines/pipeline/solutions/pipeline/features.py'\n",
    "PREPROC_PY = '../../tfx_pipelines/pipeline/solutions/pipeline/preprocessing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '../../tfx_pipelines/pipeline/solutions/pipeline/features.py': No such file or directory\n",
      "cp: cannot stat '../../tfx_pipelines/pipeline/solutions/pipeline/preprocessing.py': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp {FEATURE_PY} ./models/features.py\n",
    "!cp {PREPROC_PY} ./models/preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you run the tests in the two cells below they should fail \n",
    "because they were written for the template taxi dataset. \n",
    "\n",
    "**Exercise:** Modify the tests `features_test.py` and `preprocessing_test.py` as well as possibly the original modules until the tests pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 02:14:33.245553: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 02:14:33.245622: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberFeatures\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testNumberFeatures): 0.0s\n",
      "I1112 02:14:36.160474 139688828987200 test_util.py:1973] time(__main__.FeaturesTest.testNumberFeatures): 0.0s\n",
      "[       OK ] FeaturesTest.testNumberFeatures\n",
      "[ RUN      ] FeaturesTest.testTransformedName\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testTransformedName): 0.0s\n",
      "I1112 02:14:36.160954 139688828987200 test_util.py:1973] time(__main__.FeaturesTest.testTransformedName): 0.0s\n",
      "[       OK ] FeaturesTest.testTransformedName\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m models.features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 02:24:24.907804: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 02:24:24.907856: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] PreprocessingTest.testPreprocessingFn\n",
      "INFO:tensorflow:time(__main__.PreprocessingTest.testPreprocessingFn): 0.0s\n",
      "I1112 02:24:27.824175 140311220856640 test_util.py:1973] time(__main__.PreprocessingTest.testPreprocessingFn): 0.0s\n",
      "[       OK ] PreprocessingTest.testPreprocessingFn\n",
      "[ RUN      ] PreprocessingTest.test_session\n",
      "[  SKIPPED ] PreprocessingTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m models.preprocessing_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Change the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as for the pre-processing we want to reuse the model we develop for the covertype dataset,\n",
    "so we will simply copy it over the template model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PY  = '../../tfx_pipelines/pipeline/solutions/pipeline/model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {MODEL_PY} ./models/keras/model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Move the constants defined in `model.py` into `constants.py` and import them from `model.py` to respect the template structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Run the covertype TFX pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a TFX pipeline using the `tfx pipeline create` command.\n",
    "\n",
    "**Note:** When creating a pipeline for KFP, we need a container image which will \n",
    "be used to run our pipeline. And skaffold will build the image for us. Because `skaffold`\n",
    "pulls base images from the docker hub, it will take 5~10 minutes when we build\n",
    "the image for the first time, but it will take much less time from the second build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While creating a pipeline, `Dockerfile` and `build.yaml` will be generated to build a Docker image.\n",
    "\n",
    "Don't forget to add these files to the source control system (for example, git) along with other source files.\n",
    "\n",
    "A pipeline definition file for [argo](https://argoproj.github.io/argo/) will be generated, too. \n",
    "The name of this file is `${PIPELINE_NAME}.tar.gz.` \n",
    "For example, it will be `guided_project_2.tar.gz` if the name of your pipeline is `guided_project_1`. \n",
    "It is recommended NOT to include this pipeline definition file into source control, because it will be generated from other Python files and will be updated whenever you update the pipeline. For your convenience, this file is already listed in `.gitignore` which is generated automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start an execution run with the newly created pipeline using the `tfx run create` command.\n",
    "\n",
    "**Note:** You may see the following error `Error importing tfx_bsl_extension.coders.` Please ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can also run the pipeline in the KFP Dashboard. The new execution run will be listed \n",
    "under Experiments in the KFP Dashboard. \n",
    "Clicking into the experiment will allow you to monitor progress and visualize \n",
    "the artifacts created during the execution run.\n",
    "\n",
    "However, we recommend visiting the KFP Dashboard. You can access the KFP Dashboard from \n",
    "the Cloud AI Platform Pipelines menu in Google Cloud Console. Once you visit the dashboard, \n",
    "you will be able to find the pipeline, and access a wealth of information about the pipeline. \n",
    "For example, you can find your runs under the Experiments menu, and when you open your\n",
    "execution run under Experiments you can find all your artifacts from the pipeline under Artifacts menu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If your pipeline run fails, you can see detailed logs for each TFX component in the Experiments tab in the KFP Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major sources of failure is permission related problems. \n",
    "Please make sure your KFP cluster has permissions to access Google Cloud APIs.\n",
    "This can be configured [when you create a KFP cluster in GCP](https://cloud.google.com/ai-platform/pipelines/docs/setting-up),\n",
    "or see [Troubleshooting document in GCP](https://cloud.google.com/ai-platform/pipelines/docs/troubleshooting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Add components for data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will add components for data validation including `StatisticsGen`, `SchemaGen`, and `ExampleValidator`.\n",
    "If you are interested in data validation, please see \n",
    "[Get started with Tensorflow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to change directory to pipeline and double-click again to open** `pipeline.py`. \n",
    "Find and uncomment the 3 lines which add `StatisticsGen`, `SchemaGen`, and `ExampleValidator` to the pipeline.\n",
    "(Tip: search for comments containing TODO(step 5):). Make sure to save `pipeline.py` after you edit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now need to update the existing pipeline with modified pipeline definition. Use the `tfx pipeline update` command to update your pipeline, followed by the `tfx run create` command to create a new execution run of your updated pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can run the pipeline the same way.\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check pipeline outputs\n",
    "\n",
    "Visit the KFP dashboard to find pipeline outputs in the page for your pipeline run. Click the Experiments tab on the left, and All runs in the Experiments page. You should be able to find the latest run under the name of your pipeline.\n",
    "\n",
    "See link below to access the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://' + ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Add components for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will add components for training and model validation including `Transform`, `Trainer`, `ResolverNode`, `Evaluator`, and `Pusher`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click to open** `pipeline.py`. Find and uncomment the 5 lines which add `Transform`, `Trainer`, `ResolverNode`, `Evaluator` and `Pusher` to the pipeline. (Tip: search for TODO(step 6):)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:** \n",
    "\n",
    "* In `pipeline.py` make sure you turn the cache of by setting `enable_cache=False` for debugging purposes (otherwise components that have been previously run won't be).\n",
    "\n",
    "* In `pipeline.py`, you'll need to set `infer_feature_shape=False`, otherwise you'll run into sparse/dense tensor mismatch.\n",
    "\n",
    "* In `pipeline.py`, you'll need to set the label `big_tipper` from the default template to the right label for our dataset in \n",
    "\n",
    "```python\n",
    "tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='big_tipper')],\n",
    "```\n",
    "\n",
    "* In `configs.py`, adapt the values of the variables `TRAIN_NUM_STEPS` and `EVAL_NUM_STEPS` to match what we had in the original covertype pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did before, you now need to update the existing pipeline with the modified pipeline definition. The instructions are the same as Step 5. Update the pipeline using `tfx pipeline update`, and create an execution run using `tfx run create`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the pipeline DAG has changed accordingly in the Kubeflow UI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 02:34:18.628151: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 02:34:18.628203: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline -> gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  2.107MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.25.0\n",
      "[Skaffold]  ---> 05d9b228cf63\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 08c109d99192\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> e050f7e6b652\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 90991d60d1bf\n",
      "[Skaffold] Removing intermediate container 90991d60d1bf\n",
      "[Skaffold]  ---> 091bd3d62151\n",
      "[Skaffold] Successfully built 091bd3d62151\n",
      "[Skaffold] Successfully tagged gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]\n",
      "[Skaffold] 8e460e5f0899: Preparing\n",
      "[Skaffold] 171db4078c1d: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 171db4078c1d: Layer already exists\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 8e460e5f0899: Pushed\n",
      "[Skaffold] latest: digest: sha256:21ccdae0d53abe78499978b20adb91c6e1ced748ddb18a38c8826f1441193012 size: 3268\n",
      "[Skaffold] \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-11-12 02:34:34.029971: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 02:34:34.030025: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: BigQueryExampleGen\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2/guided_project_2.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 11, 12, 2, 34, 37, tzinfo=tzlocal()),\n",
      " 'description': None,\n",
      " 'id': 'ff961b9f-3e09-4e17-a465-db7c1c143a66',\n",
      " 'name': 'guided_project_2_20211112023437',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx_pipeline_output/guided_project_2'}],\n",
      " 'resource_references': [{'key': {'id': 'dbe9a0f1-c635-445d-b897-0872d50d89b3',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/pipelines/details/dbe9a0f1-c635-445d-b897-0872d50d89b3\n",
      "Pipeline \"guided_project_2\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com/\n"
     ]
    }
   ],
   "source": [
    "print(\"https://\" + ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 02:34:42.255797: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 02:34:42.255843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: guided_project_2\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: guided_project_2\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name    | run_id                               | status   | created_at                | link                                                                                                                          |\n",
      "+==================+======================================+==========+===========================+===============================================================================================================================+\n",
      "| guided_project_2 | 06019b03-348b-414b-bc2a-ae851a6cd215 |          | 2021-11-12T02:34:46+00:00 | https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/runs/details/06019b03-348b-414b-bc2a-ae851a6cd215 |\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this execution run finishes successfully, you have now created and run your first TFX pipeline in AI Platform Pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Try Cloud AI Platform Training and Prediction with KFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX interoperates with several managed GCP services, such as [Cloud AI Platform for Training and Prediction](https://cloud.google.com/ai-platform/). You can set your `Trainer` component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can push your model to Cloud AI Platform Prediction for serving. In this step, we will set our `Trainer` and `Pusher` component to use Cloud AI Platform services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before editing files, you might first have to enable AI Platform Training & Prediction API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double-click pipeline to change directory, and double-click to open** `configs.py`. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `GCP_AI_PLATFORM_TRAINING_ARGS` and `GCP_AI_PLATFORM_SERVING_ARGS`. We will use our custom built container image to train a model in Cloud AI Platform Training, so we should set `masterConfig.imageUri` in `GCP_AI_PLATFORM_TRAINING_ARGS` to the same value as `CUSTOM_TFX_IMAGE` above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change directory one level up, and double-click to open** `kubeflow_dag_runner.py`. Uncomment `ai_platform_training_args` and `ai_platform_serving_args`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the pipeline and create an execution run as we did in step 5 and 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 03:03:45.033927: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 03:03:45.033976: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline -> gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  2.116MB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.25.0\n",
      "[Skaffold]  ---> 05d9b228cf63\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 08c109d99192\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 22869cab51e3\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 2988e78e32f2\n",
      "[Skaffold] Removing intermediate container 2988e78e32f2\n",
      "[Skaffold]  ---> 841139737bb9\n",
      "[Skaffold] Successfully built 841139737bb9\n",
      "[Skaffold] Successfully tagged gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]\n",
      "[Skaffold] 5cd82c5eaea9: Preparing\n",
      "[Skaffold] 171db4078c1d: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] 171db4078c1d: Layer already exists\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] 5cd82c5eaea9: Pushed\n",
      "[Skaffold] latest: digest: sha256:45ccfb71d09b26d0cc6e1053a09fc11d59c48e9f1a3205289039bf4358f8594b size: 3268\n",
      "[Skaffold] \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-11-12 03:03:59.841118: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 03:03:59.841168: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2/pipeline/pipeline.py:76: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2/pipeline/pipeline.py:76: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_2/guided_project_2.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 11, 12, 3, 4, 3, tzinfo=tzlocal()),\n",
      " 'description': None,\n",
      " 'id': 'f593c355-d743-4c1c-84b2-f6fbccb483df',\n",
      " 'name': 'guided_project_2_20211112030403',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx_pipeline_output/guided_project_2'}],\n",
      " 'resource_references': [{'key': {'id': 'dbe9a0f1-c635-445d-b897-0872d50d89b3',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/pipelines/details/dbe9a0f1-c635-445d-b897-0872d50d89b3\n",
      "Pipeline \"guided_project_2\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 03:04:04.666732: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 03:04:04.666782: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: guided_project_2\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: guided_project_2\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name    | run_id                               | status   | created_at                | link                                                                                                                          |\n",
      "+==================+======================================+==========+===========================+===============================================================================================================================+\n",
      "| guided_project_2 | 044f2dc8-1586-4e33-9ca6-16bf437048db |          | 2021-11-12T03:04:08+00:00 | https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/runs/details/044f2dc8-1586-4e33-9ca6-16bf437048db |\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find your training jobs in [Cloud AI Platform Jobs](https://console.cloud.google.com/ai-platform/jobs). If your pipeline completed successfully, you can find your model in [Cloud AI Platform Models](https://console.cloud.google.com/ai-platform/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
