{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Project 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objective:**\n",
    "\n",
    "* Learn how to customize the tfx template to your own dataset\n",
    "* Learn how to modify the Keras model scaffold provided by tfx template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guided project, we will use the `tfx template` tool to create a TFX pipeline for the covertype project, but this time, instead of re-using an already implemented model as we did in guided project 2, we will adapt the model scaffold generated by `tfx template` so that it can train on the covertype dataset\n",
    "\n",
    "**Note:** The covertype dataset is loacated at\n",
    "```\n",
    "gs://workshop-datasets/covertype/small/dataset.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envirnonment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the your Kubeflow pipelines endopoint below the same way you did in guided project 1 & 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=qwiklabs-gcp-04-0ad772141888\n"
     ]
    }
   ],
   "source": [
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'\n",
    "CUSTOM_TFX_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to restart the kernel at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `skaffold` tool setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/skaffold\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "LOCAL_BIN=\"/home/jupyter/.local/bin\"\n",
    "SKAFFOLD_URI=\"https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64\"\n",
    "\n",
    "test -d $LOCAL_BIN || mkdir -p $LOCAL_BIN\n",
    "\n",
    "which skaffold || (\n",
    "    curl -Lo skaffold $SKAFFOLD_URI &&\n",
    "    chmod +x skaffold               &&\n",
    "    mv skaffold $LOCAL_BIN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the `PATH` environment variable so that `skaffold` is available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you shoud see the `skaffold` tool with the command `which`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/skaffold\n"
     ]
    }
   ],
   "source": [
    "!which skaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Copy the predefined template to your project directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will create a working pipeline project directory and \n",
    "files by copying additional files from a predefined template.\n",
    "\n",
    "You may give your pipeline a different name by changing the PIPELINE_NAME below. \n",
    "\n",
    "This will also become the name of the project directory where your files will be put."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./guided_project_3'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_NAME = 'guided_project_3' # Your pipeline name\n",
    "PROJECT_DIR = os.path.join(os.path.expanduser(\".\"), PIPELINE_NAME)\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX includes the taxi template with the TFX python package. \n",
    "\n",
    "If you are planning to solve a point-wise prediction problem,\n",
    "including classification and regresssion, this template could be used as a starting point.\n",
    "\n",
    "The `tfx template copy` CLI command copies predefined template files into your project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 03:10:06.340766: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 03:10:06.340814: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Copying taxi pipeline template\n",
      "model_analysis.ipynb -> ./guided_project_3/model_analysis.ipynb\n",
      "pipeline.py -> ./guided_project_3/pipeline/pipeline.py\n",
      "__init__.py -> ./guided_project_3/pipeline/__init__.py\n",
      "configs.py -> ./guided_project_3/pipeline/configs.py\n",
      "kubeflow_dag_runner.py -> ./guided_project_3/kubeflow_dag_runner.py\n",
      "data_validation.ipynb -> ./guided_project_3/data_validation.ipynb\n",
      "model.py -> ./guided_project_3/models/keras/model.py\n",
      "constants.py -> ./guided_project_3/models/keras/constants.py\n",
      "__init__.py -> ./guided_project_3/models/keras/__init__.py\n",
      "model_test.py -> ./guided_project_3/models/keras/model_test.py\n",
      "model.py -> ./guided_project_3/models/estimator/model.py\n",
      "constants.py -> ./guided_project_3/models/estimator/constants.py\n",
      "__init__.py -> ./guided_project_3/models/estimator/__init__.py\n",
      "model_test.py -> ./guided_project_3/models/estimator/model_test.py\n",
      "preprocessing.py -> ./guided_project_3/models/preprocessing.py\n",
      "preprocessing_test.py -> ./guided_project_3/models/preprocessing_test.py\n",
      "__init__.py -> ./guided_project_3/models/__init__.py\n",
      "features.py -> ./guided_project_3/models/features.py\n",
      "features_test.py -> ./guided_project_3/models/features_test.py\n",
      "__init__.py -> ./guided_project_3/__init__.py\n",
      "beam_dag_runner.py -> ./guided_project_3/beam_dag_runner.py\n",
      ".gitignore -> ./guided_project_3/.gitignore\n"
     ]
    }
   ],
   "source": [
    "!tfx template copy \\\n",
    "  --pipeline-name={PIPELINE_NAME} \\\n",
    "  --destination-path={PROJECT_DIR} \\\n",
    "  --model=taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Browse your copied source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TFX template provides basic scaffold files to build a pipeline, including Python source code,\n",
    "sample data, and Jupyter Notebooks to analyse the output of the pipeline. \n",
    "\n",
    "The `taxi` template uses the same Chicago Taxi dataset and ML model as \n",
    "the [Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop).\n",
    "\n",
    "Here is brief introduction to each of the Python files:\n",
    "\n",
    "`pipeline` - This directory contains the definition of the pipeline\n",
    "* `configs.py` — defines common constants for pipeline runners\n",
    "* `pipeline.py` — defines TFX components and a pipeline\n",
    "\n",
    "`models` - This directory contains ML model definitions.\n",
    "* `features.py`, `features_test.py` — defines features for the model\n",
    "* `preprocessing.py`, `preprocessing_test.py` — defines preprocessing jobs using tf::Transform\n",
    "\n",
    "`models/estimator` - This directory contains an Estimator based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using TF estimator\n",
    "\n",
    "`models/keras` - This directory contains a Keras based model.\n",
    "* `constants.py` — defines constants of the model\n",
    "* `model.py`, `model_test.py` — defines DNN model using Keras\n",
    "\n",
    "`beam_dag_runner.py`, `kubeflow_dag_runner.py` — define runners for each orchestration engine\n",
    "\n",
    "\n",
    "**Running the tests:**\n",
    "You might notice that there are some files with `_test.py` in their name. \n",
    "These are unit tests of the pipeline and it is recommended to add more unit \n",
    "tests as you implement your own pipelines. \n",
    "You can run unit tests by supplying the module name of test files with `-m` flag. \n",
    "You can usually get a module name by deleting `.py` extension and replacing `/` with `..`\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 05:10:02.571214: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 05:10:02.571270: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] FeaturesTest.testNumberFeatures\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testNumberFeatures): 0.0s\n",
      "I1112 05:10:05.385713 140033962850112 test_util.py:1973] time(__main__.FeaturesTest.testNumberFeatures): 0.0s\n",
      "[       OK ] FeaturesTest.testNumberFeatures\n",
      "[ RUN      ] FeaturesTest.testTransformedName\n",
      "INFO:tensorflow:time(__main__.FeaturesTest.testTransformedName): 0.0s\n",
      "I1112 05:10:05.386065 140033962850112 test_util.py:1973] time(__main__.FeaturesTest.testTransformedName): 0.0s\n",
      "[       OK ] FeaturesTest.testTransformedName\n",
      "[ RUN      ] FeaturesTest.test_session\n",
      "[  SKIPPED ] FeaturesTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n"
     ]
    }
   ],
   "source": [
    "!python -m models.features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 05:11:57.258555: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 05:11:57.258607: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] ModelTest.testBuildKerasModel\n",
      "INFO:tensorflow:time(__main__.ModelTest.testBuildKerasModel): 0.0s\n",
      "I1112 05:12:00.623120 139826107111232 test_util.py:1973] time(__main__.ModelTest.testBuildKerasModel): 0.0s\n",
      "[  FAILED  ] ModelTest.testBuildKerasModel\n",
      "[ RUN      ] ModelTest.test_session\n",
      "[  SKIPPED ] ModelTest.test_session\n",
      "======================================================================\n",
      "ERROR: testBuildKerasModel (__main__.ModelTest)\n",
      "ModelTest.testBuildKerasModel\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/models/keras/model_test.py\", line 30, in testBuildKerasModel\n",
      "    hidden_units=[1, 1], learning_rate=0.1)  # pylint: disable=protected-access\n",
      "TypeError: _build_keras_model() got an unexpected keyword argument 'hidden_units'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "FAILED (errors=1, skipped=1)\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m models.keras.model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4. Create the artifact store bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You probably already have completed this step in guided project 1, so you may\n",
    "may skip it if this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components in the TFX pipeline will generate outputs for each run as\n",
    "[ML Metadata Artifacts](https://www.tensorflow.org/tfx/guide/mlmd), and they need to be stored somewhere.\n",
    "You can use any storage which the KFP cluster can access, and for this example we\n",
    "will use Google Cloud Storage (GCS).\n",
    "\n",
    "Let us create this bucket if you haven't created it in guided project 1.\n",
    "Its name will be `<YOUR_PROJECT>-kubeflowpipelines-default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GCS_BUCKET_NAME = GOOGLE_CLOUD_PROJECT + '-kubeflowpipelines-default'\n",
    "GCS_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/data/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/jobs/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/staging/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx-template/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx_pipeline_output/\n",
      "gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tmp/\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{GCS_BUCKET_NAME} | grep {GCS_BUCKET_NAME} || gsutil mb gs://{GCS_BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Customize the pipeline to your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a TFX pipeline for a model using the Chicago Taxi dataset and the covertype dataset. Now it's time to put your data into the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your data can be stored anywhere your pipeline can access, including GCS, or BigQuery. You will need to modify the pipeline definition to access your data.\n",
    "\n",
    "Review the steps in guided project 1 and guided project 2 to remember what needs to be customized in full details. You'll find below a short summary of these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If your data is stored in files, modify the `DATA_PATH` in `kubeflow_dag_runner.py` and set it to the location of your files. If your data is stored in BigQuery, modify `BIG_QUERY_QUERY` in `pipeline/configs.py` to correctly query for your data.\n",
    "1. Add features in `models/features.py`\n",
    "1. Modify models/preprocessing.py to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n",
    "1. Modify `models/keras/model.py` and `models/keras/constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n",
    "1. Modify the `pipeline.py` and `configs.py` so that you can train and deploy on CAIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suggest that you take a small sample of the data, and select columns that are easy to preprocess for the sake of time. Here are a few pointers to get inspiration:\n",
    "\n",
    "* [A small Slice](https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=2020-11-26&date_received_min=2020-08-26&field=all&format=csv&no_aggs=true&size=119459) of the [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). (You'll still probably need to take only a subset of the rows and columns for the sake of fast model developpement.)\n",
    "* The [Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) that has a number of very interesting datasets.\n",
    "\n",
    "The easiest way to create a small CVS file containing your dataset in the Jupyterlab, and then upload it to in a Cloud Storage bucket. This way you'll simply use `CsvExampleGen` to connect to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/heart.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 11.1 KiB/ 11.1 KiB]                                                \n",
      "Operation completed over 1 objects/11.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp data/heart.csv gs://{GCS_BUCKET_NAME}/data/heart/heart.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 05:58:44.198074: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 05:58:44.198127: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "Target image gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline is not used. If the build spec is provided, update the target image in the build spec file build.yaml.\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline -> gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  196.1kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.25.0\n",
      "[Skaffold]  ---> 05d9b228cf63\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 08c109d99192\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> 0f88b8c03ecb\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in e714b9f38f9b\n",
      "[Skaffold] Removing intermediate container e714b9f38f9b\n",
      "[Skaffold]  ---> f019f321301d\n",
      "[Skaffold] Successfully built f019f321301d\n",
      "[Skaffold] Successfully tagged gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]\n",
      "[Skaffold] c008114a1ac6: Preparing\n",
      "[Skaffold] 171db4078c1d: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 171db4078c1d: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] c008114a1ac6: Pushed\n",
      "[Skaffold] latest: digest: sha256:4950fd2bf7d5250fa9536dde171ef517a74809a90ed5f34008ff5ce6da36250c size: 3267\n",
      "[Skaffold] \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-11-12 05:58:59.444951: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 05:58:59.445006: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/pipeline/pipeline.py:84: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/pipeline/pipeline.py:84: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/guided_project_3.tar.gz\n",
      "Pipeline \"guided_project_3\" already exists.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT} \\\n",
    "--build-target-image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 06:13:33.060627: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 06:13:33.060680: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Reading build spec from build.yaml\n",
      "[Skaffold] Generating tags...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline -> gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] Checking cache...\n",
      "[Skaffold]  - gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline: Not found. Building\n",
      "[Skaffold] Starting build...\n",
      "[Skaffold] Building [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]...\n",
      "[Skaffold] Sending build context to Docker daemon  196.1kB\n",
      "[Skaffold] Step 1/4 : FROM tensorflow/tfx:0.25.0\n",
      "[Skaffold]  ---> 05d9b228cf63\n",
      "[Skaffold] Step 2/4 : WORKDIR /pipeline\n",
      "[Skaffold]  ---> Using cache\n",
      "[Skaffold]  ---> 08c109d99192\n",
      "[Skaffold] Step 3/4 : COPY ./ ./\n",
      "[Skaffold]  ---> b18d0c7e5e3e\n",
      "[Skaffold] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"\n",
      "[Skaffold]  ---> Running in 1ee4ab2748b7\n",
      "[Skaffold] Removing intermediate container 1ee4ab2748b7\n",
      "[Skaffold]  ---> 3a92e33a839f\n",
      "[Skaffold] Successfully built 3a92e33a839f\n",
      "[Skaffold] Successfully tagged gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline:latest\n",
      "[Skaffold] The push refers to repository [gcr.io/qwiklabs-gcp-04-0ad772141888/tfx-pipeline]\n",
      "[Skaffold] f128aa1f3217: Preparing\n",
      "[Skaffold] 171db4078c1d: Preparing\n",
      "[Skaffold] 5dadc0a09248: Preparing\n",
      "[Skaffold] 8fb12d3bda49: Preparing\n",
      "[Skaffold] 2471eac28ba8: Preparing\n",
      "[Skaffold] 674ba689ae71: Preparing\n",
      "[Skaffold] 4058ae03fa32: Preparing\n",
      "[Skaffold] e3437c61d457: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] 54b00d861a7a: Preparing\n",
      "[Skaffold] c547358928ab: Preparing\n",
      "[Skaffold] 84ff92691f90: Preparing\n",
      "[Skaffold] c4e66be694ce: Preparing\n",
      "[Skaffold] 47cc65c6dd57: Preparing\n",
      "[Skaffold] 84ff92691f90: Waiting\n",
      "[Skaffold] 54b00d861a7a: Waiting\n",
      "[Skaffold] 674ba689ae71: Waiting\n",
      "[Skaffold] c547358928ab: Waiting\n",
      "[Skaffold] c4e66be694ce: Waiting\n",
      "[Skaffold] 47cc65c6dd57: Waiting\n",
      "[Skaffold] 4058ae03fa32: Waiting\n",
      "[Skaffold] e3437c61d457: Waiting\n",
      "[Skaffold] 8fb12d3bda49: Layer already exists\n",
      "[Skaffold] 5dadc0a09248: Layer already exists\n",
      "[Skaffold] 171db4078c1d: Layer already exists\n",
      "[Skaffold] 2471eac28ba8: Layer already exists\n",
      "[Skaffold] 674ba689ae71: Layer already exists\n",
      "[Skaffold] 4058ae03fa32: Layer already exists\n",
      "[Skaffold] 54b00d861a7a: Layer already exists\n",
      "[Skaffold] 84ff92691f90: Layer already exists\n",
      "[Skaffold] e3437c61d457: Layer already exists\n",
      "[Skaffold] c547358928ab: Layer already exists\n",
      "[Skaffold] 47cc65c6dd57: Layer already exists\n",
      "[Skaffold] c4e66be694ce: Layer already exists\n",
      "[Skaffold] f128aa1f3217: Pushed\n",
      "[Skaffold] latest: digest: sha256:2b3ef7fb0ad3a64224aca1de7fb6822b7effd40c9d600ae62d51e1f6fc4499bd size: 3267\n",
      "[Skaffold] \n",
      "New container image is built. Target image is available in the build spec file.\n",
      "2021-11-12 06:13:48.734668: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 06:13:48.734738: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/pipeline/pipeline.py:84: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:tensorflow:From /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/pipeline/pipeline.py:84: external_input (from tfx.utils.dsl_utils) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "external_input is deprecated, directly pass the uri to ExampleGen.\n",
      "WARNING:absl:The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "INFO:absl:Adding upstream dependencies for component CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:Adding upstream dependencies for component StatisticsGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component ExampleValidator\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: StatisticsGen\n",
      "INFO:absl:Adding upstream dependencies for component Transform\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:Adding upstream dependencies for component Trainer\n",
      "INFO:absl:   ->  Component: SchemaGen\n",
      "INFO:absl:   ->  Component: Transform\n",
      "INFO:absl:Adding upstream dependencies for component Evaluator\n",
      "INFO:absl:   ->  Component: CsvExampleGen\n",
      "INFO:absl:   ->  Component: ResolverNode_latest_blessed_model_resolver\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "INFO:absl:Adding upstream dependencies for component Pusher\n",
      "INFO:absl:   ->  Component: Evaluator\n",
      "INFO:absl:   ->  Component: Trainer\n",
      "Pipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/asl-ml-immersion/notebooks/tfx_pipelines/guided_projects/guided_project_3/guided_project_3.tar.gz\n",
      "{'code_source_url': None,\n",
      " 'created_at': datetime.datetime(2021, 11, 12, 6, 13, 52, tzinfo=tzlocal()),\n",
      " 'description': None,\n",
      " 'id': '01eb3d0b-21b6-453b-bb8c-2e5b23e28754',\n",
      " 'name': 'guided_project_3_20211112061352',\n",
      " 'package_url': None,\n",
      " 'parameters': [{'name': 'pipeline-root',\n",
      "                 'value': 'gs://qwiklabs-gcp-04-0ad772141888-kubeflowpipelines-default/tfx_pipeline_output/guided_project_3'}],\n",
      " 'resource_references': [{'key': {'id': 'ba4143c7-8c40-43b5-8f55-4d8d08dd97ee',\n",
      "                                  'type': 'PIPELINE'},\n",
      "                          'name': None,\n",
      "                          'relationship': 'OWNER'}]}\n",
      "Please access the pipeline detail page at https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/pipelines/details/ba4143c7-8c40-43b5-8f55-4d8d08dd97ee\n",
      "Pipeline \"guided_project_3\" updated successfully.\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_dag_runner.py \\\n",
    "--endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-12 06:13:53.878946: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-11-12 06:13:53.879009: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "CLI\n",
      "Creating a run for pipeline: guided_project_3\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "Run created for pipeline: guided_project_3\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n",
      "| pipeline_name    | run_id                               | status   | created_at                | link                                                                                                                          |\n",
      "+==================+======================================+==========+===========================+===============================================================================================================================+\n",
      "| guided_project_3 | 24128c6e-40e8-46eb-b592-88bc34c348fa |          | 2021-11-12T06:13:58+00:00 | https://164002aec59f7c0d-dot-us-central1.pipelines.googleusercontent.com//#/runs/details/24128c6e-40e8-46eb-b592-88bc34c348fa |\n",
      "+------------------+--------------------------------------+----------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "\n",
    "Copyright 2021 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
